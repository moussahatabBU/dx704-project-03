{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_avVIGTpvioI"
      },
      "source": [
        "# DX 704 Week 3 Project\n",
        "\n",
        "This week's project will give you practice with optimizing choices for bandit algorithms.\n",
        "You will be given access to the bandit problem via a blackbox object, and you will investigate the bandit rewards to pick a suitable algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftRkegOQowWA"
      },
      "source": [
        "The full project description, a template notebook and supporting code are available on GitHub: [Project 3 Materials](https://github.com/bu-cds-dx704/dx704-project-03).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tPHvNSEdR6h"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6VKDAEY8JMI"
      },
      "source": [
        "## Part 1: Pick a Bandit Algorithm\n",
        "\n",
        "Experiment with the multi-armed bandit interface using seed 0 to learn about the distribution of rewards and decide what kind of bandit algorithm will be appropriate.\n",
        "A histogram will likely be helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "BCggNE7NpiQN"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class BanditProblem(object):\n",
        "    def __init__(self, seed):\n",
        "        self.seed = seed\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.num_arms = 3\n",
        "        self.ns = self.rng.integers(low=1, high=10, size=self.num_arms)\n",
        "        self.ps = self.rng.uniform(low=0.2, high=0.4, size=self.num_arms)\n",
        "\n",
        "    def get_num_arms(self):\n",
        "        return self.num_arms\n",
        "\n",
        "    def get_reward(self, arm):\n",
        "        if arm < 0 or arm >= self.num_arms:\n",
        "            raise ValueError(\"Invalid arm\")\n",
        "\n",
        "        x = self.rng.uniform()\n",
        "        x *= self.rng.binomial(self.ns[arm], self.ps[arm])\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "X99ZQUyhpgak"
      },
      "outputs": [],
      "source": [
        "bandit0 = BanditProblem(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "frDtVjt4qATJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bandit0.get_num_arms()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sdM9Ec3HqC6h"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.8255111545554434"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bandit0.get_reward(arm=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "iuQ0jCr_plcZ"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import log, sqrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upper Confidence Bound\n",
        "\n",
        "def run_ucb(bandit, T=3000, R_max=9.0):\n",
        "    num_arms = bandit.get_num_arms()\n",
        "    counts = np.zeros(num_arms, dtype=int)\n",
        "    sums = np.zeros(num_arms, dtype=float)\n",
        "\n",
        "    for a in range(num_arms):\n",
        "        r = bandit.get_reward(a) / R_max\n",
        "        counts[a] += 1\n",
        "        sums[a] += r\n",
        "\n",
        "    total = counts.sum()\n",
        "    while total < T:\n",
        "        means = sums / np.maximum(counts, 1)\n",
        "        bonuses = np.sqrt(2.0 * np.log(total + 1) / counts)\n",
        "        a = int(np.argmax(means + bonuses))\n",
        "        r = bandit.get_reward(a) / R_max\n",
        "        counts[a] += 1\n",
        "        sums[a] += r\n",
        "        total += 1\n",
        "\n",
        "    avg_reward = (sums.sum() / T) * R_max   \n",
        "    return avg_reward, counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thomspon sampling\n",
        "def run_thompson_sampling(bandit, T=3000):\n",
        "    num_arms = bandit.get_num_arms()\n",
        "    container = [[] for _ in range(K)]\n",
        "    counts = [0]*num_arms\n",
        "\n",
        "    for a in range(num_arms):\n",
        "        r = bandit.get_reward(a)\n",
        "        container[a].append(r); counts[a] += 1\n",
        "\n",
        "    for _ in range(sum(counts), T):\n",
        "        boot_means = [np.mean(np.random.choice(container[a], size=len(container[a]), replace=True)) for a in range(K)]\n",
        "        a = int(np.argmax(boot_means))\n",
        "        container[a].append(bandit.get_reward(a)); counts[a] += 1\n",
        "\n",
        "    avg_reward = np.mean([x for reward_a in container for x in reward_a])\n",
        "    return avg_reward, np.array(counts), np.array([np.mean(reward_a) for reward_a in container])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Epsilon greedy\n",
        "\n",
        "def run_epsilon_greedy(bandit, T=3000, eps0=0.2, decay=0.997):\n",
        "    num_arms = bandit.get_num_arms()\n",
        "    counts = np.zeros(K, dtype=int)\n",
        "    sums = np.zeros(K, dtype=float)\n",
        "    eps = eps0\n",
        "\n",
        "    for a in range(num_arms):\n",
        "        r = bandit.get_reward(a)\n",
        "        counts[a] += 1\n",
        "        sums[a] += r\n",
        "\n",
        "    total = counts.sum()\n",
        "    while total < T:\n",
        "        if np.random.random() < eps:\n",
        "            a = np.random.randint(num_arms)\n",
        "        else:\n",
        "            a = int(np.argmax(sums / np.maximum(counts, 1)))\n",
        "        r = bandit.get_reward(a)\n",
        "        counts[a] += 1\n",
        "        sums[a] += r\n",
        "        total += 1\n",
        "        eps *= decay\n",
        "\n",
        "    avg_reward = sums.sum() / T\n",
        "    return avg_reward, counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Result evaluation \n",
        "\n",
        "def eval_results(runs=20, T=3000):\n",
        "\n",
        "    ucb_avgs, eg_avgs, ts_avgs = [], [], []\n",
        "\n",
        "    for _ in range(runs):\n",
        "        ucb_avgs.append(run_ucb(BanditProblem(0), T=T, R_max=9.0)[0])\n",
        "        eg_avgs.append(run_epsilon_greedy(BanditProblem(0), T=T)[0])\n",
        "        ts_avgs.append(run_thompson_sampling(BanditProblem(0), T=T)[0])\n",
        "\n",
        "    print(f\"UCB   mean={np.mean(ucb_avgs):.4f} ± {np.std(ucb_avgs):.4f}\")\n",
        "    print(f\"e-greedy mean={np.mean(eg_avgs):.4f} ± {np.std(eg_avgs):.4f}\")\n",
        "    print(f\"TS mean={np.mean(ts_avgs):.4f} ± {np.std(ts_avgs):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UCB   mean=0.7994 ± 0.0000\n",
            "e-greedy mean=0.8849 ± 0.0168\n",
            "TS mean=0.8804 ± 0.0077\n"
          ]
        }
      ],
      "source": [
        "eval_results(20, 3000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4-3KtNXtzlY"
      },
      "source": [
        "Based on your investigation, pick an appropriate bandit algorithm to implement from the algorithms covered this week.\n",
        "Write a file \"algorithm-choice.txt\" that states your choice and gives a few sentences justifying your choice and rejecting the alternatives.\n",
        "Keep your explanation concise; overly verbose responses will be penalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY_xvfK4rN0C"
      },
      "source": [
        "## Part 2: Implement Bandit\n",
        "\n",
        "Based on your decision, implement an appropriate bandit algorithm and pick 1000 actions using seed 2025002."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kufc5pAPrWTT"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "def thompson_sampling(env_seed=2025002, T=1000, algo_seed=2025002):\n",
        "\n",
        "    bandit = BanditProblem(env_seed)\n",
        "    rng = np.random.default_rng(algo_seed)\n",
        "\n",
        "    num_arms = bandit.get_num_arms()\n",
        "    container = [[] for _ in range(num_arms)]\n",
        "    actions, rewards = [], []\n",
        "\n",
        "    for a in range(num_arms):\n",
        "        r = bandit.get_reward(a)\n",
        "        container[a].append(r)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "    for _ in range(num_arms, T):\n",
        "        boot_means = []\n",
        "        for a in range(num_arms):\n",
        "            reward_a = np.asarray(container[a], dtype=float)\n",
        "            idx = rng.integers(0, len(reward_a), size=len(reward_a)) \n",
        "            boot_means.append(reward_a[idx].mean())\n",
        "        a_star = int(np.argmax(boot_means))\n",
        "\n",
        "        r = bandit.get_reward(a_star)\n",
        "        container[a_star].append(r)\n",
        "        actions.append(a_star)\n",
        "        rewards.append(r)\n",
        "\n",
        "    counts = np.array([len(container[a]) for a in range(K)])\n",
        "    avg_reward = float(np.mean(rewards))\n",
        "    return actions, rewards, counts, avg_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions, rewards, counts, avg_reward = thompson_sampling(env_seed=2025002, T=1000, algo_seed=2025002)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(actions, rewards, counts, avg_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho9QihatrZqy"
      },
      "source": [
        "Write a file \"history.tsv\" with columns action and reward in the order that the actions were taken."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "OsiU7S1XrX0q"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "history = pd.DataFrame({\"action\": actions, \"reward\": rewards}).to_csv(\"history.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwm-1x3mrfXu"
      },
      "source": [
        "Submit \"history.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc0xYgCzrmGj"
      },
      "source": [
        "## Part 3: Action Statistics\n",
        "\n",
        "Based on the data from part 2, estimate the expected reward for each arm and write a file \"actions.tsv\" with the columns action, min_reward, mean_reward, max_reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "a-uAbY03sFna"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "hist = pd.read_csv(\"history.tsv\", sep=\"\\t\")\n",
        "\n",
        "stats = (\n",
        "    hist.groupby(\"action\", as_index=False)[\"reward\"]\n",
        "        .agg(min_reward=\"min\", mean_reward=\"mean\", max_reward=\"max\")\n",
        "        .sort_values(\"action\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats.to_csv(\"actions.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk8s1hpEsHWX"
      },
      "source": [
        "Submit \"actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asaIrLTtsKEv"
      },
      "source": [
        "## Part 4: Regret Estimates\n",
        "\n",
        "Calculate the expected regret taking 1000 actions with the following strategies.\n",
        "\n",
        "* uniform: Pick an arm uniformly at random.\n",
        "* just-i: Always pick arm $i$. Do this for $i=0$ to $K-1$ where $K$ is the number of arms.\n",
        "* actual: This should match your output in part 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "LgCSJKDmso5a"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "env = BanditProblem(2025002)\n",
        "num_arms = env.get_num_arms()\n",
        "mu = 0.5 * env.ns * env.ps\n",
        "mu_star = float(mu.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "hist = pd.read_csv(\"history.tsv\", sep=\"\\t\")\n",
        "len_hist = len(hist)\n",
        "counts = hist[\"action\"].value_counts().reindex(range(num_arms), fill_value=0).to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = [\n",
        "    (\"uniform\", len_hist * (mu_star - float(mu.mean()))),\n",
        "    *[(f\"just-{i}\", len_hist * (mu_star - float(mu[i]))) for i in range(K)],\n",
        "    (\"actual\", len_hist * mu_star - float(np.dot(counts, mu))),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncXs2IqPsqQO"
      },
      "source": [
        "Write your results to a file \"strategies.tsv\" with the columns strategy and regret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "GlYK-oCUtyFm"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "strat = pd.DataFrame(rows, columns=[\"strategy\", \"regret\"]).to_csv(\"strategies.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNs9BJCvtz2N"
      },
      "source": [
        "Submit \"strategies.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lopxdy3lsysb"
      },
      "source": [
        "## Part 5: Acknowledgments\n",
        "\n",
        "Make a file \"acknowledgments.txt\" documenting any outside sources or help on this project.\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for.\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy.\n",
        "If no acknowledgements are appropriate, just write none in the file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8-GaDpOw06W"
      },
      "source": [
        "Submit \"acknowledgments.txt\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AR_XyZi8N_Q"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXhGo_008M-b"
      },
      "source": [
        "Submit \"project.ipynb\" in Gradescope."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
